# README
# Phillip Long
# November 6, 2023

# Get a list of all expressive features in the relevant portion of the musescore dataset, count up each type

# python /home/pnlong/model_musescore/discretize_expressive_features.py


# IMPORTS
##################################################

import pickle
import multiprocessing
from os.path import exists
from os import remove
from tqdm import tqdm
from time import perf_counter, strftime, gmtime
import pandas as pd
import argparse
import logging
from re import sub
from representation import clean_up_text as data_text_clean
from utils import rep

##################################################


# CONSTANTS
##################################################

INPUT_FILEPATH = "/data2/pnlong/musescore/expressive_features.csv"
OUTPUT_FILEPATH = "/data2/pnlong/musescore/discrete_expressive_features.csv"

OUTPUT_COLUMNS = ["type", "feature"]
NA_VALUE = "NA"
MISSING_VALUE = "no value"
MAX_DISCRETE_VALUES_TO_CARE_ABOUT = 1000 # when displaying discretized values, display a maximum of this number if there are more than that

##################################################


# HELPER FUNCTIONS
##################################################

# get rid of html tags, quotation marks
def clean_up_text(text: str) -> str:

    # remove random whitespace
    text = sub(pattern = r"\s+", repl = " ", string = text)

    # clean up html tags
    while text.find("<") >= 0 and text.find(">") >= 0:
        text = text[:text.find("<")] + text[text.find(">") + 1:]
    
    # remove quotations
    text = sub(pattern = "\"|'", repl = "", string = text)

    # trim on both ends
    text = text.strip()

    return text

##################################################

# ARGUMENTS
##################################################

def parse_args(args = None, namespace = None):
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(prog = "Discretize", description = "Get list of MuseScore expressive feature subtypes and text.")
    parser.add_argument("-i", "--input_filepath", type = str, default = INPUT_FILEPATH, help = "CSV File with expressive feature information generated by parse_mscz.py")
    parser.add_argument("-f", "--output_filepath", type = str, default = OUTPUT_FILEPATH, help = "Where to output data")
    parser.add_argument("-j", "--jobs", type = int, default = int(multiprocessing.cpu_count() / 4), help = "Number of Jobs")
    return parser.parse_args(args = args, namespace = namespace)

##################################################


# GIVEN PICKLE PATH, EXTRACT INFO
##################################################

def extract_information(path: str, output_filepath: str):

    # what kinds of expressive text are present?

    # OPEN PICKLE FILE, DO NECESSARY WRANGLING
    ##################################################

    # unpickle
    try:
        with open(path, "rb") as pickle_file:
            unpickled = pickle.load(file = pickle_file)
    except (OSError):
        return None
    
    # extract expressive features dataframe
    expressive_features = unpickled.pop("expressive_features") # ("time", "measure", "duration", "type", "feature", "comment")
    del unpickled

    # remove spanner subfix
    # expressive_features["type"] = expressive_features["type"].apply(lambda type_: type_.replace("Spanner", ""))

    # fix missing feature values
    expressive_features["feature"] = expressive_features["feature"].apply(lambda feature: feature if feature is not None else MISSING_VALUE)

    # extract specific columns
    expressive_features = expressive_features[OUTPUT_COLUMNS]

    expressive_features.to_csv(path_or_buf = output_filepath, sep = ",", na_rep = NA_VALUE, header = False, index = False, mode = "a")

    ##################################################
    
##################################################


# MAIN CLASS FOR MULTIPROCESSING
##################################################

if __name__ == "__main__":

    # CONSTANTS
    ##################################################
    args = parse_args()

    logging.basicConfig(level = logging.INFO, format = "%(message)s")
    ##################################################


    # GET EXPRESSIVE FEATURES
    ##################################################

    if not exists(args.output_filepath):
        
        # temporary output filepath
        temp_output_filepath = args.output_filepath.split(".")
        temp_output_filepath.insert(1, "temp")
        temp_output_filepath = ".".join(temp_output_filepath)

        # load in data
        data = pd.read_csv(filepath_or_buffer = args.input_filepath, sep = ",", header = 0, index_col = False)

        # filter data
        data = data[data["is_valid"] & data["is_public_domain"] & (data["n_expressive_features"] > 0) & (data["expressive_features"].apply(lambda expressive_features_path: exists(str(expressive_features_path))))].reset_index(drop = True)

        # write columns
        pd.DataFrame(columns = OUTPUT_COLUMNS).to_csv(path_or_buf = temp_output_filepath, sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w")

        # parse through data with multiprocessing
        logging.info(f"N_PATHS = {len(data)}") # print number of paths to process
        chunk_size = 1
        start_time = perf_counter() # start the timer
        with multiprocessing.Pool(processes = args.jobs) as pool:
            results = pool.starmap(func = extract_information,
                                   iterable = tqdm(iterable = zip(data["expressive_features"], rep(x = temp_output_filepath, times = len(data))), desc = "Obtaining Expressive Features", total = len(data)),
                                   chunksize = chunk_size)
        end_time = perf_counter() # stop the timer
        total_time = end_time - start_time # compute total time elapsed
        total_time = strftime("%H:%M:%S", gmtime(total_time)) # convert into pretty string
        logging.info(f"Total time: {total_time}")

        data = pd.read_csv(filepath_or_buffer = temp_output_filepath, sep = ",", na_values = NA_VALUE, header = 0, index_col = False)

        data = data.groupby(by = OUTPUT_COLUMNS, as_index = False).size()
        # data = data[data["feature"] != str(None)] # filter out None values
        data["feature"] = data["feature"].apply(clean_up_text)
        data = data.sort_values(by = ["type", "size"], ascending = False)
        data.to_csv(path_or_buf = args.output_filepath, sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w")
        remove(temp_output_filepath)
        logging.info(f"Saved output to {args.output_filepath}")
    
    else: # if we have already created the file, load it in

        data = pd.read_csv(filepath_or_buffer = args.output_filepath, sep = ",", header = 0, index_col = False)

    ##################################################

    
    # WRITE OUT DISCRETIZED EXPRESSIVE FEATURE TYPES
    ##################################################

    # relevant expressive feature types we care about extracting
    expressive_feature_types = pd.unique(values = data["type"])
    # expressive_feature_types = {"GraceNote", "Barline", "TimeSignature", "KeySignature", "Tempo", "Text", "RehearsalMark", "Dynamic", "HairPin", "Fermata", "TechAnnotation", "Symbol", "Articulation", "Slur", "Pedal"}
    output = {expressive_feature_type: ["", 0] for expressive_feature_type in expressive_feature_types}

    # create ouptut
    for expressive_feature_type in expressive_feature_types:
        expressive_feature_type_subtypes = data[data["type"] == expressive_feature_type]["feature"].tolist()
        if expressive_feature_type in ("Text", "TextSpanner", "RehearsalMark", "HairPinSpanner", "TempoSpanner"):
            expressive_feature_type_subtypes = list(dict.fromkeys([data_text_clean(text = str(expressive_feature_type_subtype)) for expressive_feature_type_subtype in expressive_feature_type_subtypes]))
        output[expressive_feature_type][1] = len(expressive_feature_type_subtypes) # for sorting by this value later
        n_expressive_feature_subtypes = str(output[expressive_feature_type][1])
        # if len(expressive_feature_type_subtypes) == 0:
        #     continue
        if len(expressive_feature_type_subtypes) > MAX_DISCRETE_VALUES_TO_CARE_ABOUT: # if there are too many values to care about
            expressive_feature_type_subtypes = expressive_feature_type_subtypes[:MAX_DISCRETE_VALUES_TO_CARE_ABOUT] # filter them down
            # n_expressive_feature_subtypes = f"{MAX_DISCRETE_VALUES_TO_CARE_ABOUT}+"
        output[expressive_feature_type][0] += f"# {expressive_feature_type} ({n_expressive_feature_subtypes})\n" # comment
        output[expressive_feature_type][0] += ", ".join((f"\"{expressive_feature_type_subtype}\"" for expressive_feature_type_subtype in expressive_feature_type_subtypes)) + ",\n" # info
    output = [item[0] for item in sorted(output.values(), key = lambda item: item[1], reverse = True)] # sort from most discrete values to least

    # write to file
    output_filepath_txt = args.output_filepath.split(".")[0] + ".txt"
    with open(output_filepath_txt, "w") as file:
        file.writelines(output)
    logging.info(f"Wrote info to {output_filepath_txt}")

    ##################################################

##################################################
