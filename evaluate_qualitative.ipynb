{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation of Model Performance\n",
    "\n",
    "Given the same prefix sequence of notes, does adding expressive features change model output? That is, do models respect expressive features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists, basename\n",
    "from os import makedirs, mkdir, remove\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import representation\n",
    "from typing import Tuple\n",
    "from IPython.display import display, HTML, Audio\n",
    "from time import sleep\n",
    "import utils\n",
    "import torch\n",
    "import dataset\n",
    "import music_x_transformers\n",
    "import train\n",
    "import encode\n",
    "import decode\n",
    "from tqdm import tqdm\n",
    "from read_mscz.read_mscz import read_musescore\n",
    "from read_mscz.music import MusicExpress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Some constants like filepaths and encodings for running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data directory\n",
    "DATA_DIR = \"/home/pnlong/musescore/datav\"\n",
    "DEVICE = \"cpu\"\n",
    "SHOW_SEQUENCES = False\n",
    "\n",
    "# filepaths\n",
    "TEST_DATA_DIR = \"/home/pnlong/musescore/test_data/evalqual\"\n",
    "if not exists(TEST_DATA_DIR):\n",
    "    makedirs(TEST_DATA_DIR)\n",
    "PREFIX_MSCZ_FILEPATH = f\"{TEST_DATA_DIR}/simple.mscz\"\n",
    "if not exists(PREFIX_MSCZ_FILEPATH):\n",
    "    raise FileNotFoundError(\"Must provide a valid MuseScore prefix filepath.\")\n",
    "PREFIX_OUTPUT = basename(PREFIX_MSCZ_FILEPATH).split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Encoding Data\n",
    "\n",
    "Various encoding-related data such as the use of absolute time and velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoding\n",
    "encoding = representation.load_encoding(filepath = f\"{DATA_DIR}/encoding.json\")\n",
    "\n",
    "# some special tokens and variables\n",
    "include_velocity = encoding[\"include_velocity\"]\n",
    "use_absolute_time = encoding[\"use_absolute_time\"]\n",
    "type_dim = encoding[\"dimensions\"].index(\"type\")\n",
    "sos = encoding[\"type_code_map\"][\"start-of-song\"]\n",
    "eos_type_string = \"end-of-song\"\n",
    "eos = encoding[\"type_code_map\"][eos_type_string]\n",
    "note_token, grace_note_token = encoding[\"type_code_map\"][\"note\"], encoding[\"type_code_map\"][\"grace-note\"]\n",
    "expressive_feature_token = encoding[\"type_code_map\"][representation.EXPRESSIVE_FEATURE_TYPE_STRING]\n",
    "_, unidimensional_decoding_function = representation.get_unidimensional_coding_functions(encoding = encoding)\n",
    "\n",
    "# define the device to load models on\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "# helper function for displaying sequences\n",
    "def show(sequence: torch.tensor, columns: list = encoding[\"dimensions\"], show_index: bool = True, show_sequences: bool = SHOW_SEQUENCES):\n",
    "    \"\"\"Display data tables.\"\"\"\n",
    "    if not show_sequences:\n",
    "        return\n",
    "    if len(sequence.shape) == 3:\n",
    "        sequence = sequence.squeeze(0)\n",
    "    sequence = pd.DataFrame(data = sequence, columns = columns)\n",
    "    # sequence.style.hide(axis = \"index\")\n",
    "    display(HTML(sequence.to_html(index = show_index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Prefix Sequence\n",
    "\n",
    "Prepare the prefix sequence by extracting relevant data from the MuseScore file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save prepared prefix sequence filepaths to\n",
    "paths = f\"{TEST_DATA_DIR}/paths.txt\"\n",
    "remove(paths)\n",
    "\n",
    "# helper function to prepare a prefix\n",
    "def prepare_prefix(music: MusicExpress, prefix_path: str, include_expressive_features: bool = True):\n",
    "\n",
    "    # extract data from MusicExpress object\n",
    "    music.tracks = [music.tracks[0],] # make sure it is just one track\n",
    "    data = encode.extract_data(music = music, use_implied_duration = True, include_velocity = include_velocity, use_absolute_time = use_absolute_time)\n",
    "    if (not include_expressive_features):\n",
    "        data = data[data[:, type_dim] != representation.EXPRESSIVE_FEATURE_TYPE_STRING]\n",
    "    data = data[data[:, type_dim] != eos_type_string] # remove end of song token\n",
    "    show(sequence = data, columns = representation.DIMENSIONS)\n",
    "\n",
    "    # save encoded data\n",
    "    np.save(file = prefix_path, arr = data)\n",
    "\n",
    "    # text file with just the prefix path inside\n",
    "    with open(paths, \"a\") as paths_output:\n",
    "        paths_output.write(prefix_path + \"\\n\")\n",
    "\n",
    "# get MusicExpress object, both normal and expressive-feature-realized\n",
    "music = read_musescore(path = PREFIX_MSCZ_FILEPATH, timeout = 10)\n",
    "for track in music.tracks:\n",
    "    for note in track.notes:\n",
    "        note.velocity = track.notes[0].velocity\n",
    "prepare_prefix(music = music, prefix_path = f\"{TEST_DATA_DIR}/{PREFIX_OUTPUT}.npy\", include_expressive_features = False)\n",
    "music = read_musescore(path = PREFIX_MSCZ_FILEPATH, timeout = 10)\n",
    "music.realize_expressive_features()\n",
    "prepare_prefix(music = music, prefix_path = f\"{TEST_DATA_DIR}/{PREFIX_OUTPUT}.realized.npy\", include_expressive_features = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a Model\n",
    "\n",
    "Given the model name, generate two samples:\n",
    "\n",
    "- One with a prefix of only constant-velocity notes.\n",
    "- One with a prefix of notes and expressive features.\n",
    "\n",
    "Do the two samples differ? Does the latter respect expressive feature markings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Given the model name (`model`), generate two samples with a prefix of...\n",
    "    - just notes at constant velocity.\n",
    "    - notes and expressive features.\n",
    "    Returns the filepaths to both samples.\n",
    "    \n",
    "    Do the two samples differ? Does the expressive feature sample respect the provided expressive features?\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD IN MODEL\n",
    "    ##################################################\n",
    "    # Load in a model.\n",
    "\n",
    "    # get directories\n",
    "    model_dir = f\"{DATA_DIR}/{model}\"\n",
    "    if not exists(model_dir):\n",
    "        raise FileNotFoundError(f\"{model_dir} does not exist.\")\n",
    "    evalqual_output_dir = f\"{model_dir}/evalqual\"\n",
    "    if not exists(evalqual_output_dir):\n",
    "        mkdir(evalqual_output_dir)\n",
    "\n",
    "    # load training configurations\n",
    "    train_args = utils.load_json(filepath = f\"{model_dir}/train_args.json\")\n",
    "\n",
    "    # create the dataset\n",
    "    max_seq_len = train_args[\"max_seq_len\"]\n",
    "    conditioning = train_args[\"conditioning\"]\n",
    "    unidimensional = train_args.get(\"unidimensional\", False)\n",
    "    type_dim = encoding[\"unidimensional_encoding_order\" if unidimensional else \"dimensions\"].index(\"type\")\n",
    "    is_anticipation = (conditioning == encode.CONDITIONINGS[-1])\n",
    "    sigma = train_args[\"sigma\"]\n",
    "    test_dataset = dataset.MusicDataset(paths = paths, encoding = encoding, conditioning = conditioning, max_seq_len = max_seq_len, use_augmentation = False, is_baseline = False, unidimensional = unidimensional)\n",
    "\n",
    "    # create the model\n",
    "    print(\"Creating model...\")\n",
    "    model = music_x_transformers.MusicXTransformer(\n",
    "        dim = train_args[\"dim\"],\n",
    "        encoding = encoding,\n",
    "        depth = train_args[\"layers\"],\n",
    "        heads = train_args[\"heads\"],\n",
    "        max_seq_len = max_seq_len,\n",
    "        max_temporal = encoding[\"max_\" + (\"time\" if use_absolute_time else \"beat\")],\n",
    "        rotary_pos_emb = train_args[\"rel_pos_emb\"],\n",
    "        use_abs_pos_emb = train_args[\"abs_pos_emb\"],\n",
    "        emb_dropout = train_args[\"dropout\"],\n",
    "        attn_dropout = train_args[\"dropout\"],\n",
    "        ff_dropout = train_args[\"dropout\"],\n",
    "        unidimensional = unidimensional,\n",
    "    ).to(device)\n",
    "\n",
    "    # load the checkpoint\n",
    "    checkpoint_filepath = f\"{model_dir}/checkpoints/best_model.{train.PARTITIONS[1]}.pth\"\n",
    "    model.load_state_dict(state_dict = torch.load(f = checkpoint_filepath, map_location = device))\n",
    "    print(f\"Loaded model weights from: {checkpoint_filepath}\")\n",
    "    model.eval()\n",
    "\n",
    "    # create data loader, get the singular batch\n",
    "    test_data_loader = torch.utils.data.DataLoader(dataset = test_dataset, num_workers = 4, collate_fn = dataset.MusicDataset.collate, batch_size = 1, shuffle = False)\n",
    "    test_iter = iter(test_data_loader)\n",
    "    \n",
    "    ##################################################\n",
    "\n",
    "\n",
    "    # GENERATE SAMPLES\n",
    "    ##################################################\n",
    "\n",
    "    # get prefix\n",
    "    prefix = next(test_iter)[\"seq\"]\n",
    "\n",
    "    # generate\n",
    "    generated = model.generate(\n",
    "        seq_in = prefix,\n",
    "        seq_len = train.DEFAULT_MAX_SEQ_LEN,\n",
    "        eos_token = eos,\n",
    "        temperature = 1.0,\n",
    "        filter_logits_fn = \"top_k\",\n",
    "        filter_thres = 0.9,\n",
    "        monotonicity_dim = (\"type\", \"time\" if use_absolute_time else \"beat\"),\n",
    "        notes_only = True,\n",
    "        is_anticipation = is_anticipation,\n",
    "        sigma = sigma\n",
    "    )\n",
    "\n",
    "    # helper function to synthesize a sequence\n",
    "    suffixes = [\"note\", \"total\"]\n",
    "    audio_output_filepaths = tuple(f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.{suffix}.wav\" for suffix in suffixes)\n",
    "    def synthesize_generation(notes_only: bool = True):\n",
    "        \"\"\"Synthesize the generated sequence as audio, saving the sequence in the process.\"\"\"\n",
    "        i = int(not notes_only)\n",
    "        show(sequence = generated[i], show_sequences = False)\n",
    "        generation = torch.cat(tensors = (prefix[i], generated[i]), dim = 0).numpy() # wrangle a bit\n",
    "        np.save(file = f\"{evalqual_output_dir}/{PREFIX_OUTPUT}.{suffixes[i]}.npy\", arr = generation) # save as a numpy array\n",
    "\n",
    "        # convert to audio\n",
    "        music = decode.decode(codes = generation, encoding = encoding, unidimensional_decoding_function = unidimensional_decoding_function) # convert to a MusicExpress object\n",
    "        music.write(path = audio_output_filepaths[i])\n",
    "    \n",
    "    # save audios\n",
    "    synthesize_generation(notes_only = True)\n",
    "    synthesize_generation(notes_only = False)\n",
    "\n",
    "    # return audio filepaths\n",
    "    return audio_output_filepaths\n",
    "\n",
    "    ##################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples\n",
    "\n",
    "Get the available models in the provided data directory, then generate samples for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_output_filepath = f\"{DATA_DIR}/evalqual.csv\"\n",
    "\n",
    "# create if samples are not generated yet\n",
    "if not exists(samples_output_filepath):\n",
    "\n",
    "    # generate a list of available models\n",
    "    with open(f\"{DATA_DIR}/models.txt\", \"r\") as models_output: # read in list of trained models\n",
    "        models = [model.strip() for model in models_output.readlines()]\n",
    "        models = sorted(models)\n",
    "\n",
    "    # generate samples\n",
    "    audio_outputs_note, audio_outputs_total = utils.rep(x = \"\", times = len(models)), utils.rep(x = \"\", times = len(models))\n",
    "    for i in (progress_bar := tqdm(iterable = range(len(models)), desc = \"Generating samples for models\")):\n",
    "        progress_bar.set_postfix(model = f\"{models[i]}\")\n",
    "        audio_outputs_note[i], audio_outputs_total[i] = evaluate(model = models[i])\n",
    "    samples = pd.DataFrame(data = {\"model\": models, \"note\": audio_outputs_note, \"total\": audio_outputs_total})\n",
    "    samples.to_csv(path_or_buf = samples_output_filepath, sep = \",\", na_rep = train.NA_VALUE, header = True, index = False, mode = \"w\") # write data frame\n",
    "\n",
    "    del models, audio_outputs_note, audio_outputs_total\n",
    "\n",
    "# if samples have previously been generated\n",
    "else:\n",
    "\n",
    "    # load in the data frame\n",
    "    samples = pd.read_csv(filepath_or_buffer = samples_output_filepath, sep = \",\", na_values = train.NA_VALUE, header = 0, index_col = False)\n",
    "\n",
    "# display\n",
    "display(HTML(samples.to_html(index = False))) # show data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate the Models\n",
    "\n",
    "Listen to the `.wav` files -- did adding expressive features make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ratings\n",
    "print(\"Playing generated samples, please rate 1-5.\\nEnter 0 if the sample is incoherent to sanity check the baseline.\")\n",
    "ratings = [None] * len(samples)\n",
    "for i, model in enumerate(samples[\"model\"]):\n",
    "    display(Audio(samples.at[i, \"total\"]))\n",
    "    rating = input(f\"Rate {model}: \")\n",
    "    if len(rating) == 0:\n",
    "        continue\n",
    "    while True:\n",
    "        try:\n",
    "            rating = int(rating)\n",
    "            break\n",
    "        except ValueError:\n",
    "            rating = input(\"Please enter a numeric value: \")\n",
    "    if (1 <= rating) and (rating <= 5):\n",
    "        ratings[i] = rating\n",
    "    else:\n",
    "        print(f\"Playing baseline version of {model}: \")\n",
    "        display(Audio(samples.at[i, \"total\"]))\n",
    "        sleep(seconds = 2)\n",
    "samples[\"rating\"] = ratings\n",
    "\n",
    "# save rating to file\n",
    "ratings_output_filepath = f\"{DATA_DIR}/ratings.csv\"\n",
    "samples.to_csv(path_or_buf = ratings_output_filepath, sep = \",\", na_rep = train.NA_VALUE, header = True, index = False, mode = \"w\") # write data frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
