# README
# Phillip Long
# October 16, 2023

# Make plots that describe expressive features in musescore files.

# python /home/pnlong/model_musescore/expressive_features_plots.py


# IMPORTS
##################################################

import pickle
import multiprocessing
from os.path import exists
from os import makedirs, remove
from tqdm import tqdm
from time import perf_counter, strftime, gmtime
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import argparse
import logging
from parse_mscz import TIME_IN_SECONDS_COLUMN_NAME
from parse_mscz_plots import OUTPUT_DIR, OUTPUT_RESOLUTION_DPI, GREY
from read_mscz.music import DIVIDE_BY_ZERO_CONSTANT
from utils import rep, split_camel_case
from train import NA_VALUE
from copy import deepcopy

plt.style.use("default")
plt.rcParams["font.family"] = "serif"
plt.rcParams["mathtext.fontset"] = "dejavuserif"

##################################################


# CONSTANTS
##################################################

INPUT_FILEPATH = "/data2/pnlong/musescore/expressive_features/expressive_features.csv"
FILE_OUTPUT_DIR = "/data2/pnlong/musescore/expressive_features"

LARGE_PLOTS_DPI = int(1.5 * OUTPUT_RESOLUTION_DPI)
ALL_FEATURES_TYPE_NAME = "AllTypes" # name of all features plot name
CUTOFF_PERCENTILE = 95

# COLORS
TEXT_GREEN = "#5e813f"
TEMPORAL_PURPLE = "#68349a"
SPANNER_BLUE = "#4f71be"
DYNAMIC_GOLD = "#b89230"
SYMBOL_ORANGE = "#ff624c"
SYSTEM_SALMON = "#a91b0d"
ALL_BROWN = "#964b00"
EXPRESSIVE_FEATURE_COLORS = {
    ALL_FEATURES_TYPE_NAME: ALL_BROWN,
    "Lyric": TEXT_GREEN,
    "Tempo": TEMPORAL_PURPLE,
    "Dynamic": DYNAMIC_GOLD,
    "TimeSignature": SYSTEM_SALMON,
    "Text": TEXT_GREEN,
    "KeySignature": SYSTEM_SALMON,
    "Barline": SYSTEM_SALMON,
    "Articulation": SYMBOL_ORANGE,
    "HairPin": DYNAMIC_GOLD,
    "RehearsalMark": TEXT_GREEN,
    "Slur": SPANNER_BLUE,
    "Fermata": TEMPORAL_PURPLE,
    "Pedal": SPANNER_BLUE,
}

# columns of data tables
RELEVANT_TIME_UNITS = ["time_steps", "seconds", "bars", "beats"]
SONG_LENGTH_COLUMNS = ["path", "track"] + RELEVANT_TIME_UNITS
DENSITY_COLUMNS = ["path"] + RELEVANT_TIME_UNITS
FEATURE_TYPES_SUMMARY_COLUMNS = ["path", "type", "size"]
SPARSITY_SUCCESSIVE_SUFFIX = "_successive"
SPARSITY_COLUMNS = ["path", "type", "value", "time_steps", "seconds", "beats", "fraction"]
DISTANCE_COLUMNS = SPARSITY_COLUMNS[SPARSITY_COLUMNS.index("time_steps"):]
SUCCESSIVE_DISTANCE_COLUMNS = [distance_column + SPARSITY_SUCCESSIVE_SUFFIX for distance_column in DISTANCE_COLUMNS]
SPARSITY_COLUMNS += SUCCESSIVE_DISTANCE_COLUMNS

##################################################


# ARGUMENTS
##################################################

def parse_args(args = None, namespace = None):
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(prog = "Expressive Feature Figures", description = "Make plots describing expressive features in MuseScore dataset.")
    parser.add_argument("-i", "--input_filepath", type = str, default = INPUT_FILEPATH, help = "CSV File with expressive feature information generated by parse_mscz.py")
    parser.add_argument("-f", "--file_output_dir", type = str, default = FILE_OUTPUT_DIR, help = "Where to store data tables created for the plots")
    parser.add_argument("-o", "--output_dir", type = str, default = OUTPUT_DIR, help = "Output directory")
    parser.add_argument("-r", "--reset", action = "store_true", help = "Do we remake the relevant data tables for the plots?") 
    parser.add_argument("-j", "--jobs", type = int, default = int(multiprocessing.cpu_count() / 4), help = "Number of Jobs")
    return parser.parse_args(args = args, namespace = namespace)

##################################################


# HELPER FUNCTION FOR INFO EXTRACTION
##################################################
# helper function to calculate difference between successive rows
def calculate_difference_between_successive_entries(df: pd.DataFrame, columns: list) -> pd.DataFrame:
    """Calculates the difference between successive features."""
    df = df.copy().sort_values(by = columns[0]) # sort values
    df_values = df[columns] # store values of columns
    df.loc[:, columns] = np.nan # set columns' values to None
    df.loc[df.index[:-1], columns] = df_values.loc[df.index[1:], columns].to_numpy() - df_values.loc[df.index[:-1], columns].to_numpy() # calculate differences
    return df
##################################################


# GIVEN PICKLE PATH, EXTRACT INFO
##################################################

def extract_information(path: str):
    """Given a path to a pickled expressive-feature information file, extract some information on said file.

    what kinds of expressive text are present?
    what is the relative distance between successive expression markings (both in notes and in seconds)?
      - if markings are relatively dense we can sub select sequences
      - if they're sparse than it really depends on what kinds of expression they are

    """

    # OPEN PICKLE FILE, DO NECESSARY WRANGLING
    ##################################################

    # unpickle
    try:
        with open(path, "rb") as pickle_file:
            unpickled = pickle.load(file = pickle_file)
    except (OSError):
        return None
    
    # extract expressive features dataframe
    expressive_features = unpickled.pop("expressive_features")

    # wrangle expressive features
    expressive_features["type"] = expressive_features["type"].apply(lambda expressive_feature_type: expressive_feature_type.replace("Spanner", ""))

    ##################################################

    
    # SONG LENGTH
    ##################################################
    
    song_length = {time_unit: song_length_in_time_unit for time_unit, song_length_in_time_unit in unpickled["track_length"].items()}
    song_length["path"] = path.split(".")[0] + ".mscz"
    song_length["track"] = int(path.split(".")[1])
    song_lenth = pd.DataFrame(data = [song_length], columns = SONG_LENGTH_COLUMNS)
    song_lenth.to_csv(path_or_buf = list(PLOT_DATA_OUTPUT_FILEPATHS.values())[0], sep = ",", na_rep = NA_VALUE, header = False, index = False, mode = "a")

    ##################################################

    
    # CALCULATE DENSITY OF EXPRESSIVE FEATURES
    ##################################################

    density = {time_unit: (song_length_in_time_unit / len(expressive_features)) for time_unit, song_length_in_time_unit in unpickled["track_length"].items()} # time unit per expressive feature
    density["path"] = path
    density = pd.DataFrame(data = [density], columns = DENSITY_COLUMNS)
    density.to_csv(path_or_buf = list(PLOT_DATA_OUTPUT_FILEPATHS.values())[1], sep = ",", na_rep = NA_VALUE, header = False, index = False, mode = "a")

    ##################################################


    # SUMMARIZE TYPES OF TEXT/FEATURES PRESENT
    ##################################################

    feature_types_summary = expressive_features[["type", "value"]].groupby(by = "type", as_index = False).size() # group by type
    feature_types_summary["path"] = rep(x = path, times = len(feature_types_summary))
    feature_types_summary = feature_types_summary[FEATURE_TYPES_SUMMARY_COLUMNS] # ensure we have just the columns we need
    feature_types_summary.to_csv(path_or_buf = list(PLOT_DATA_OUTPUT_FILEPATHS.values())[2], sep = ",", na_rep = NA_VALUE, header = False, index = False, mode = "a")

    ##################################################


    # RELATIVE DISTANCE BETWEEN SUCCESSIVE MARKINGS
    ##################################################

    # add some columns, set up for calculation of distance
    sparsity = expressive_features[["type", "value", "time", TIME_IN_SECONDS_COLUMN_NAME]]
    sparsity = sparsity.rename(columns = {"time": "time_steps", TIME_IN_SECONDS_COLUMN_NAME: "seconds"})
    sparsity["path"] = rep(x = path, times = len(sparsity))
    sparsity["beats"] = sparsity["time_steps"] / unpickled["resolution"]
    sparsity["fraction"] = sparsity["time_steps"] / (unpickled["track_length"]["time_steps"] + DIVIDE_BY_ZERO_CONSTANT)
    for successive_distance_column, distance_column in zip(SUCCESSIVE_DISTANCE_COLUMNS, DISTANCE_COLUMNS): # add successive times columns
        sparsity[successive_distance_column] = sparsity[distance_column]
    sparsity = sparsity[SPARSITY_COLUMNS].sort_values(by = "time_steps").reset_index(drop = True) # sort by increasing times

    # calculate distances
    expressive_feature_types = pd.unique(expressive_features["type"]) # get types of expressive features
    distance = calculate_difference_between_successive_entries(df = sparsity, columns = DISTANCE_COLUMNS)
    for expressive_feature_type in expressive_feature_types: # get distances between successive features of the same type
        distance_for_expressive_feature_type = calculate_difference_between_successive_entries(df = sparsity[sparsity["type"] == expressive_feature_type], columns = SUCCESSIVE_DISTANCE_COLUMNS) # calculate sparsity for certain feature type
        distance.loc[distance_for_expressive_feature_type.index, SUCCESSIVE_DISTANCE_COLUMNS] = distance_for_expressive_feature_type[SUCCESSIVE_DISTANCE_COLUMNS].to_numpy()
    distance.to_csv(path_or_buf = list(PLOT_DATA_OUTPUT_FILEPATHS.values())[3], sep = ",", na_rep = NA_VALUE, header = False, index = False, mode = "a")

    ##################################################
    
##################################################


# GET SONG LENGTH STATISTICS
##################################################

def get_song_length_statistics(input_filepath: str) -> dict:
    """Returns a dictionary with various statistics on song length."""

    # load in table
    song_length = pd.read_csv(filepath_or_buffer = input_filepath, sep = ",", header = 0, index_col = False)

    # create statistics dictionary
    statistics = {relevant_time_unit: {"total_track_length": 0, "total_song_length": 0, "mean_song_length": 0, "median_song_length": 0} for relevant_time_unit in RELEVANT_TIME_UNITS}

    # get total track length, where each track is counted independently, for different time units
    for relevant_time_unit in statistics.keys():
        statistics[relevant_time_unit]["total_track_length"] = sum(song_length[relevant_time_unit])

    # filter so that this is by song, not track
    song_length = song_length.drop_duplicates(subset = "path", keep = "first", ignore_index = True) # drop duplicates because all tracks from the same song will have the same length

    # get totals by song for different time units
    for relevant_time_unit in RELEVANT_TIME_UNITS:
        statistics[relevant_time_unit]["total_song_length"] = sum(song_length[relevant_time_unit])
        statistics[relevant_time_unit]["mean_song_length"] = np.mean(song_length[relevant_time_unit])
        statistics[relevant_time_unit]["median_song_length"] = np.median(song_length[relevant_time_unit])

    return statistics

##################################################


# DENSITY
##################################################

def make_density_plot(input_filepath: str, output_filepath: str) -> None:

    relevant_density_types = ["seconds", "bars", "beats"]

    # create figure
    fig, axes = plt.subplot_mosaic(mosaic = list(map(list, zip(relevant_density_types))), constrained_layout = True, figsize = (8, 8))
    density = pd.read_csv(filepath_or_buffer = input_filepath, sep = ",", header = 0, index_col = False)
    fig.suptitle(f"Expressive Feature [EF] Densities in Public-Domain Tracks ({len(density):,} total)", fontweight = "bold")

    # create plot
    n_bins = 40
    relevant_ranges = {relevant_density_types[0]: (0, 40), relevant_density_types[1]: (0, 40), relevant_density_types[2]: (0, 40)}
    for i in range(len(relevant_density_types) - 1, -1, -1): # traverse backwards
        relevant_density_type = relevant_density_types[i]
        n_points_excluded = len(density[~((density[relevant_density_type] >= relevant_ranges[relevant_density_type][0]) & (density[relevant_density_type] <= relevant_ranges[relevant_density_type][1]))])
        axes[relevant_density_type].hist(x = density[relevant_density_type], bins = n_bins, range = relevant_ranges[relevant_density_type], orientation = "horizontal", log = False, color = GREY) # , color = COLORS[i], edgecolor = "0"
        axes[relevant_density_type].set_ylabel(relevant_density_type.title())
        axes[relevant_density_type].set_title(f"{relevant_density_type.title()} per EF ({n_points_excluded:,} excluded, Mean: {np.mean(density[relevant_density_type]):.2f}, Median: {np.median(density[relevant_density_type]):.2f})")
        axes[relevant_density_type].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}"))
        if i < len(relevant_density_types) - 1: # remove x axis stuff for top 2 columns
            axes[relevant_density_type].sharex(axes[relevant_density_types[-1]])
        else:
            axes[relevant_density_type].set_xlabel("Count")
    
    # remove x axis for upper plots
    # for relevant_density_type in relevant_density_types[:-1]:
    #     axes[relevant_density_type].set_xticks([])
    #     axes[relevant_density_type].set_xticklabels([])

    # save image
    fig.savefig(output_filepath, dpi = OUTPUT_RESOLUTION_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Density plot saved to {output_filepath}.")

    # clear up memory
    del density

    # return none
    return None

##################################################


# SUMMARIZE FEATURE TYPES PRESENT
##################################################

def make_summary_plot(input_filepath: str, output_filepath_prefix: str) -> list:

    # SUMMARY BAR CHART

    # create figure
    plot_types = ("total", "mean", "median")
    fig, axes = plt.subplot_mosaic(mosaic = [list(plot_types)], constrained_layout = True, figsize = (12, 8))
    fig.suptitle("Expressive Features", fontweight = "bold")

    # load in table
    data = pd.read_csv(filepath_or_buffer = input_filepath, sep = ",", header = 0, index_col = False)
    summary = data.drop(columns = "path")
    # summary = summary[summary["type"] != "TimeSignature"] # exclude time signatures
    summary = {
        plot_types[0]: summary.groupby(by = "type", as_index = False).sum().sort_values(by = "size"),
        plot_types[1]: summary.groupby(by = "type", as_index = False).mean().sort_values(by = "size"),
        plot_types[2]: summary.groupby(by = "type", as_index = False).median().sort_values(by = "size")
        }
    expressive_features = summary[plot_types[0]]["type"].tolist()
    # summary[plot_types[0]]["size"] = summary[plot_types[0]]["size"].apply(lambda count: np.log10(count + DIVIDE_BY_ZERO_CONSTANT)) # apply log scale to total count

    # create plot
    for i, plot_type in enumerate(plot_types):
        axes[plot_type].xaxis.grid(True)
        axes[plot_type].barh(y = summary[plot_type]["type"], width = summary[plot_type]["size"], color = GREY) # , color = COLORS[i], edgecolor = "0"
        axes[plot_type].set_title(f"{plot_type.title()}")
        axes[plot_type].ticklabel_format(axis = "x", style = "scientific", scilimits = (-1, 3))
        if i == 0: # if the left most plot
            axes[plot_type].set_xlabel(f"{plot_type.title()} Count")
            axes[plot_type].set_ylabel("Expressive Feature")
            axes[plot_type].set_yticks(axes[plot_type].get_yticks())
            axes[plot_type].set_yticklabels(axes[plot_type].get_yticklabels(), rotation = 30)
        else:
            axes[plot_type].set_xlabel(f"{plot_type.title()} Amount per Track")
            # axes[plot_type].sharey(axes[plot_types[0]]) # sharey makes it so we cant remove axis ticks and labels
            axes[plot_type].set_yticks([])
            axes[plot_type].set_yticklabels([])        

    # save image
    output_filepath = f"{output_filepath_prefix}.bar.pdf"
    fig.savefig(output_filepath, dpi = OUTPUT_RESOLUTION_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Summary Bar plot saved to {output_filepath}.")


    # SUMMARY HISTOGRAM

    # data wrangle
    n_expressive_feature_types_to_include = -1 # -1 to account for the all feature types plot
    n_col = 7
    expressive_features_for_histogram = deepcopy(expressive_features)
    # expressive_features_for_histogram.remove("Lyric")
    plot_types = [ALL_FEATURES_TYPE_NAME] + expressive_features_for_histogram[::-1][:(n_expressive_feature_types_to_include if n_expressive_feature_types_to_include > 0 else len(expressive_features))]
    mosaic = [plot_types[i:(i + n_col)] for i in range(0, len(plot_types), n_col)]
    mosaic[-1] += rep(x = "", times = n_col - len(mosaic[-1]))

    # create figure
    plot_size_factor = 1.8
    fig, axes = plt.subplot_mosaic(mosaic = mosaic, constrained_layout = True, figsize = (n_col * plot_size_factor * 1.3, len(mosaic) * plot_size_factor), empty_sentinel = "")
    # fig.suptitle("Expressive Features", fontweight = "bold")

    # print out table for paper, no longer needed
    # summary_for_paper = summary[plot_types[0]][::-1].reset_index(drop = True)
    # summary_for_paper.to_csv(path_or_buf = f"{output_filepath_prefix}.csv", sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w") # write mean values per expressive feature
    # print(f"{ALL_FEATURES_TYPE_NAME} & {sum(summary_for_paper['size']) / 1000:,.2f} \\\\")
    # for i in summary_for_paper.index:
    #     print(f"{summary_for_paper.at[i, 'type']} & {summary_for_paper.at[i, 'size'] / 1000:,.2f} \\\\")

    # plot histograms
    n_bins = 15
    axes_labels_fontsize = 14
    def get_expressive_feature_title(expressive_feature_type: str, amount: int = 0) -> str:
        """Helper function to get the title of a subplot in the summary histograms plot."""
        title = split_camel_case(string = expressive_feature_type, sep = " ").title()
        title = r"$\bf{" + r"}$ $\bf{".join(title.split(" ")) + r"}$"
        title = title + f" ({amount / 1000:,.2f})"
        return title
    for i, expressive_feature_type in enumerate(plot_types):
        if expressive_feature_type == ALL_FEATURES_TYPE_NAME:
            counts = data.drop(columns = "type").groupby(by = "path", as_index = False).sum()["size"].tolist()
        else:
            counts = data[data["type"] == expressive_feature_type]["size"].tolist()
        amount = sum(counts)
        cutoff = np.percentile(a = counts, q = CUTOFF_PERCENTILE, axis = None)
        counts = list(filter(lambda count: count <= cutoff, counts))
        axes[expressive_feature_type].hist(x = counts, bins = min(n_bins, max(counts)), log = True, align = "mid", histtype = "stepfilled", color = EXPRESSIVE_FEATURE_COLORS[expressive_feature_type])
        column_index = i % n_col
        if (i >= len(plot_types) - n_col):
            axes[expressive_feature_type].set_xlabel("Amount per Track") # only set an x label for the bottom row, fontsize = axes_labels_fontsize
        else:
            axes[expressive_feature_type].set_xticks([])
            axes[expressive_feature_type].set_xticklabels([])
        # x axis
        min_count, max_count = min(counts), max(counts)
        step = max(1, int((max_count - min_count) / 3))
        xticks = list(range(min_count, max_count + step, step))
        axes[expressive_feature_type].set_xticks(xticks)
        axes[expressive_feature_type].set_xticklabels(xticks)
        # y axis
        if (column_index == 0): # if a left most plot
            axes[expressive_feature_type].set_ylabel("Frequency") # only set a y label for the leftmost row, fontsize = axes_labels_fontsize
        axes[expressive_feature_type].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda y_value, _: f"{int(y_value):,}")) # make ticks look nice
        # title
        axes[expressive_feature_type].set_title(label = get_expressive_feature_title(expressive_feature_type = expressive_feature_type, amount = amount)) # set title

    # save image
    output_filepath = f"{output_filepath_prefix}.histogram.pdf"
    fig.savefig(output_filepath, dpi = OUTPUT_RESOLUTION_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Summary Histogram plot saved to {output_filepath}.")

    # return the order from most common to least
    return expressive_features

##################################################


# SHOW SPARSITY OF EXPRESSIVE FEATURES
##################################################

def make_sparsity_plot(input_filepath: str, output_filepath_prefix: str, expressive_feature_types: list, apply_log_percentiles_by_feature: bool = True, apply_log_histogram: bool = True, apply_log_percentiles_by_type: bool = True) -> None:

    # hyper parameters
    relevant_time_units = ["beats", "seconds"]
    relevant_time_units_suffix = [relevant_time_unit + SPARSITY_SUCCESSIVE_SUFFIX for relevant_time_unit in relevant_time_units]
    plot_types = ["total", "mean", "median"]
    output_filepaths = [f"{output_filepath_prefix}.{suffix}.pdf" for suffix in ("percentiles", "histograms", "percentiles2")]

    # we have distances between successive expressive features in time_steps, beats, seconds, and as a fraction of the length of the song
    sparsity = pd.read_csv(filepath_or_buffer = input_filepath, sep = ",", header = 0, index_col = False)
    # sparsity = sparsity.drop(index = sparsity.index[-1]) # last row is None, since there is no successive expressive features, so drop it
    sparsity = sparsity.drop(columns = "value") # we don't need this column

    # calculate percentiles, save in pickle
    expressive_feature_types.insert(0, ALL_FEATURES_TYPE_NAME) # add a plot for all expressive features
    step = 0.001
    percentiles = np.arange(start = 0, stop = 100 + step, step = step)
    pickle_output = input_filepath.split(".")[0] + "_percentiles.pickle"
    if not exists(pickle_output):

        # helper function to calculate various percentiles
        def calculate_percentiles(df: pd.DataFrame, columns: list) -> tuple:
            n = len(df) # get number of points
            df = df[~pd.isna(df[columns[0]])] # filter out NA values
            df = df[["path"] + columns] # filter down to only necessary columns
            out_columns = [column.replace(SPARSITY_SUCCESSIVE_SUFFIX, "") for column in columns] # get rid of suffix if necessary
            out = dict(zip(plot_types, rep(x = pd.DataFrame(columns = out_columns), times = len(plot_types)))) # create output dataframe
            for plot_type in plot_types:
                if plot_type == "mean":
                    df_temp = df.groupby(by = "path").mean()
                elif plot_type == "median":
                    df_temp = df.groupby(by = "path").median()
                else:
                    df_temp = df
                for column, out_column in zip(columns, out_columns):
                    out[plot_type][out_column] = np.percentile(a = df_temp[column], q = percentiles)
            return (out, n)
        percentile_values = {
            expressive_feature_type: calculate_percentiles(
                df = sparsity[sparsity["type"] == expressive_feature_type],
                columns = relevant_time_units_suffix
                ) 
                for expressive_feature_type in tqdm(iterable = [eft for eft in expressive_feature_types if eft != ALL_FEATURES_TYPE_NAME], desc = "Calculating Sparsity Percentiles")
            }
        percentile_values[ALL_FEATURES_TYPE_NAME] = calculate_percentiles(df = sparsity, columns = relevant_time_units)
    
        # save to pickle file
        with open(pickle_output, "wb") as pickle_file:
            pickle.dump(obj = percentile_values, file = pickle_file, protocol = pickle.HIGHEST_PROTOCOL)

    else: # if the pickle already exists, reload it
        with open(pickle_output, "rb") as pickle_file:
            percentile_values = pickle.load(file = pickle_file)

    # create paper figure for sparsity
    is_boxplot = True # if false, a violin plot
    alpha = 0.92
    linewidth = 1
    fig, axes = plt.subplot_mosaic(mosaic = [["sparsity"]], constrained_layout = True, figsize = (5, 3.3))
    axes["sparsity"].xaxis.grid(True)
    relevant_time_unit = relevant_time_units[0]
    sparsity_values = [[],] * len(expressive_feature_types)
    for i, expressive_feature_type in enumerate(expressive_feature_types):
        if expressive_feature_type == ALL_FEATURES_TYPE_NAME:
            current_values = sparsity[["path", relevant_time_unit]]
        else:
            current_values = sparsity[sparsity["type"] == expressive_feature_type][["path", relevant_time_unit + SPARSITY_SUCCESSIVE_SUFFIX]].rename(columns = {relevant_time_unit + SPARSITY_SUCCESSIVE_SUFFIX: relevant_time_unit}) # get subset of sparsity
        # current_values = current_values.groupby(by = "path", as_index = False, dropna = True).mean()
        current_values = current_values[relevant_time_unit].dropna().tolist()
        cutoff = np.percentile(a = current_values, q = CUTOFF_PERCENTILE, axis = None)
        sparsity_values[i] = list(filter(lambda current_value: current_value <= cutoff, current_values))
    if is_boxplot:
        boxplot = axes["sparsity"].boxplot(x = sparsity_values, vert = False, showfliers = False, patch_artist = True)
        for box, color in zip(boxplot["boxes"], [EXPRESSIVE_FEATURE_COLORS[expressive_feature_type] for expressive_feature_type in expressive_feature_types]): # set fill colors
            box.set(facecolor = color, alpha = alpha)
        for box in boxplot["medians"]: # make medians black
            box.set(color = "0")
    else:
        violin = axes["sparsity"].violinplot(sparsity_values, widths = 0.8, vert = False, showextrema = False, showmeans = False, showmedians = True)
        for violin_part, color in zip(violin["bodies"], [EXPRESSIVE_FEATURE_COLORS[expressive_feature_type] for expressive_feature_type in expressive_feature_types]):
            violin_part.set_facecolor(color)
            violin_part.set_edgecolor("0")
            violin_part.set_linewidth(linewidth)
            violin_part.set_alpha(alpha)
        for part_name in list(violin.keys())[1:]:
            violin_part = violin[part_name]
            violin_part.set_edgecolor("0")
            violin_part.set_linewidth(linewidth)
    # axis labels
    axes["sparsity"].set_xlabel(f"{' '.join(relevant_time_unit.split('_')).title()}")
    # axes["sparsity"].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x_value, _: f"{x_value:,.2f}")) # add commas
    axes["sparsity"].set_ylabel("Expression Text Type")
    axes["sparsity"].set_yticks(list(range(1, len(expressive_feature_types) + 1)))
    axes["sparsity"].set_yticklabels(list(map(lambda expressive_feature_type: split_camel_case(string = expressive_feature_type, sep = " ").title(), expressive_feature_types)), rotation = 0)
    axes["sparsity"].invert_yaxis() # so we follow the order we see in other plots
    # save image
    output_filepath_paper = f"{output_filepath_prefix}.paper.pdf"
    fig.savefig(output_filepath_paper, dpi = OUTPUT_RESOLUTION_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Sparsity Paper plot saved to {output_filepath_paper}.")

    # create figure of percentiles (faceted by expressive_feature)
    use_twin_axis_percentiles = False
    n_cols = 5
    plot_mosaic = [expressive_feature_types[i:i + n_cols] for i in range(0, len(expressive_feature_types), n_cols)] # create plot grid
    if len(plot_mosaic[-1]) < len(plot_mosaic[-2]):
        plot_mosaic[-1] += rep(x = "legend", times = (len(plot_mosaic[-2]) - len(plot_mosaic[-1])))
    is_bottom_plot = lambda expressive_feature_type: expressive_feature_types.index(expressive_feature_type) >= len(expressive_feature_types) - n_cols
    is_left_plot = lambda expressive_feature_type: expressive_feature_types.index(expressive_feature_type) % n_cols == 0
    fig, axes = plt.subplot_mosaic(mosaic = plot_mosaic, constrained_layout = True, figsize = (24, 16))
    fig.suptitle("Sparsity of Expressive Features", fontweight = "bold")
    # create percentile plot
    for expressive_feature_type in expressive_feature_types:
        axes[expressive_feature_type].grid(True) # add gridlines
        if use_twin_axis_percentiles:
            percentile_right_axis = axes[expressive_feature_type].twinx() # create twin x
        for i, plot_type in enumerate(plot_types): # plot lines
            percentiles_values_current = percentile_values[expressive_feature_type][0][plot_type].sort_values(by = relevant_time_units[0])
            if apply_log_percentiles_by_feature: # apply log function
                for column in relevant_time_units:
                    logging.debug(f"{expressive_feature_type}:{column}:{sum(percentiles_values_current[column] < 0)}-{100 * (sum(percentiles_values_current[column] < 0) / len(percentiles)):.2f}%")
                    percentiles_values_current[column] = np.log10(abs(percentiles_values_current[column]) + DIVIDE_BY_ZERO_CONSTANT)
            axes[expressive_feature_type].plot(percentiles, percentiles_values_current[relevant_time_units[0]], label = plot_type.title()) # , color = LINE_COLORS[i], linestyle = LINESTYLES[i]
            if use_twin_axis_percentiles:
                percentile_right_axis.plot(percentiles, percentiles_values_current[relevant_time_units[1]], label = plot_type.title()) # , color = LINE_COLORS[i], linestyle = LINESTYLES[i]
        if is_left_plot(expressive_feature_type = expressive_feature_type) or use_twin_axis_percentiles:
            axes[expressive_feature_type].set_ylabel(f"log({relevant_time_units[0].title()})" if apply_log_percentiles_by_feature else f"{relevant_time_units[0].title()}")
            axes[expressive_feature_type].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}")) # add commas
        else:
            axes[expressive_feature_type].sharey(axes[expressive_feature_types[int(expressive_feature_types.index(expressive_feature_type) / n_cols)]])
            axes[expressive_feature_type].set_yticklabels([])
        if use_twin_axis_percentiles:
            percentile_right_axis.set_ylabel(f"log({relevant_time_units[1].title()})" if apply_log_percentiles_by_feature else f"{relevant_time_units[1].title()}") # add
            percentile_right_axis.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}")) # add commas
        if is_bottom_plot(expressive_feature_type = expressive_feature_type): # if bottom plot, add x labels
            axes[expressive_feature_type].set_xlabel("Percentile (%)")
        else: # is not a bottom plot
            # axes[expressive_feature_type].set_xticks([]) # will keep xticks for now
            axes[expressive_feature_type].set_xticklabels([])
        axes[expressive_feature_type].set_title(f"{split_camel_case(string = expressive_feature_type, sep = ' ').title()} (n = {percentile_values[expressive_feature_type][1]:,})")
    # add a legend
    handles, labels = axes[expressive_feature_types[0]].get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    axes["legend"].legend(handles = by_label.values(), labels = by_label.keys(), loc = "center", fontsize = "x-large", title_fontsize = "xx-large", alignment = "center", mode = "expand", title = "Type")
    axes["legend"].axis("off")
    # save image
    fig.savefig(output_filepaths[0], dpi = LARGE_PLOTS_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Sparsity Percentiles plot saved to {output_filepaths[0]}.")

    # create new plot of histograms
    use_twin_axis_histogram = False
    n_bins = 15
    histogram_range = (0, 256) # in beats
    fig, axes = plt.subplot_mosaic(mosaic = plot_mosaic, constrained_layout = True, figsize = (24, 16))
    fig.suptitle("Sparsity of Expressive Features", fontweight = "bold")
    # create histogram plot
    for expressive_feature_type in expressive_feature_types:
        axes[expressive_feature_type].grid(True) # add gridlines
        if use_twin_axis_histogram:
            histogram_right_axis = axes[expressive_feature_type].twinx() # create twin x
        histogram_values = sparsity[["path"] + relevant_time_units] if expressive_feature_type == ALL_FEATURES_TYPE_NAME else sparsity[sparsity["type"] == expressive_feature_type][["path"] + relevant_time_units_suffix].rename(columns = dict(zip(relevant_time_units_suffix, relevant_time_units))) # get subset of sparsity
        histogram_values_current = dict(zip(relevant_time_units, rep(x = dict(zip(plot_types, rep(x = [], times = len(plot_types)))), times = len(relevant_time_units))))
        for i, plot_type in enumerate(plot_types): # plot lines
            histogram_values_current_current = histogram_values
            if plot_type == "mean":
                histogram_values_current_current = histogram_values.groupby(by = "path").mean()
            elif plot_type == "median":
                histogram_values_current_current = histogram_values.groupby(by = "path").median()
            for relevant_time_unit in relevant_time_units:
                histogram_values_current[relevant_time_unit][plot_type] = list(filter(lambda value: value >= histogram_range[0] and value <= histogram_range[1], histogram_values_current_current[relevant_time_unit].tolist())) # extract list
        axes[expressive_feature_type].hist(histogram_values_current[relevant_time_units[0]].values(), bins = n_bins, orientation = "horizontal", histtype = "bar", log = apply_log_histogram, label = [plot_type.title() for plot_type in plot_types], linewidth = 0.5) # plot , color = histogram_colors[:len(plot_types)], edgecolor = "0"
        if use_twin_axis_histogram:
            histogram_right_axis.hist(histogram_values_current[relevant_time_units[1]].values(), bins = n_bins, orientation = "horizontal", histtype = "bar", log = apply_log_histogram, label = [plot_type.title() for plot_type in plot_types], linewidth = 0.5) # plot , color = histogram_colors[:len(plot_types)], edgecolor = "0"
        axes[expressive_feature_type].set_ylabel(relevant_time_units[0].title())
        axes[expressive_feature_type].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}")) # add commas
        if use_twin_axis_histogram:
            histogram_right_axis.set_ylabel(relevant_time_units[1].title())
            histogram_right_axis.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}")) # add commas
        if is_bottom_plot(expressive_feature_type = expressive_feature_type):
            axes[expressive_feature_type].set_xlabel("Count")
        axes[expressive_feature_type].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: "$10^{" + str(int(np.log10(count))) + "}$" if int(np.log10(count)) >= 3 and apply_log_histogram else f"{int(count):,}")) # add commas
        # axes[expressive_feature_type].set_xticks(axes[expressive_feature_type].get_xticks())
        # axes[expressive_feature_type].set_xticklabels(axes[expressive_feature_type].get_xticklabels(), rotation = 0)
        axes[expressive_feature_type].set_title(f"{split_camel_case(string = expressive_feature_type, sep = ' ').title()} (n = {percentile_values[expressive_feature_type][1]:,})")
    # add a legend
    handles, labels = axes[expressive_feature_types[0]].get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    axes["legend"].legend(handles = by_label.values(), labels = by_label.keys(), loc = "center", fontsize = "x-large", title_fontsize = "xx-large", alignment = "center", mode = "expand", title = "Type")
    axes["legend"].axis("off")
    # save image
    fig.savefig(output_filepaths[1], dpi = LARGE_PLOTS_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Sparsity Histogram plot saved to {output_filepaths[1]}.")

    # new plot of percentiles on different facets (by plot type)
    use_twin_axis_percentiles = False
    n_rows = 2
    n_cols = int(((len(plot_types) - 1) / n_rows)) + 1
    plot_mosaic = [plot_types[i:i + n_cols] for i in range(0, len(plot_types), n_cols)] # create plot grid
    if len(plot_mosaic[-1]) < len(plot_mosaic[-2]):
        plot_mosaic[-1] += rep(x = "legend", times = (len(plot_mosaic[-2]) - len(plot_mosaic[-1])))
    else:
        for i in range(len(plot_mosaic)):
            plot_mosaic[i].append("legend")
    is_bottom_plot = lambda plot_type: plot_types.index(plot_type) >= len(plot_types) - n_cols
    is_left_plot = lambda plot_type: plot_types.index(plot_type) % n_cols == 0
    fig, axes = plt.subplot_mosaic(mosaic = plot_mosaic, constrained_layout = True, figsize = (8, 8))
    fig.suptitle("Sparsity of Expressive Features", fontweight = "bold")
    # create percentile plot
    for plot_type in plot_types:
        axes[plot_type].grid(True) # add gridlines
        if use_twin_axis_percentiles:
            percentile_right_axis = axes[plot_type].twinx() # create twin x
        for i, expressive_feature_type in enumerate(expressive_feature_types):
            percentiles_values_current = percentile_values[expressive_feature_type][0][plot_type].sort_values(by = relevant_time_units[0])
            if apply_log_percentiles_by_type: # apply log function
                for column in relevant_time_units:
                    logging.debug(f"{expressive_feature_type}:{column}:{sum(percentiles_values_current[column] < 0)}-{100 * (sum(percentiles_values_current[column] < 0) / len(percentiles)):.2f}%")
                    percentiles_values_current[column] = np.log10(abs(percentiles_values_current[column]) + DIVIDE_BY_ZERO_CONSTANT)
            axes[plot_type].plot(percentiles, percentiles_values_current[relevant_time_units[0]], label = expressive_feature_type) # , color = LINE_COMBINATIONS[i][0], linestyle = LINE_COMBINATIONS[i][1]
            if use_twin_axis_percentiles:
                percentile_right_axis.plot(percentiles, percentiles_values_current[relevant_time_units[1]], label = expressive_feature_type) # , color = LINE_COMBINATIONS[i][0], linestyle = LINE_COMBINATIONS[i][1]
        if is_left_plot(plot_type = plot_type) or use_twin_axis_percentiles:
            axes[plot_type].set_ylabel(f"log({relevant_time_units[0].title()})" if apply_log_percentiles_by_type else f"{relevant_time_units[0].title()}")
            axes[plot_type].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}")) # add commas
        else:
            axes[plot_type].sharey(axes[plot_types[int(plot_types.index(plot_type) / 2)]])
            axes[plot_type].set_yticklabels([])
        if use_twin_axis_percentiles:
            percentile_right_axis.set_ylabel(f"log({relevant_time_units[1].title()})" if apply_log_percentiles_by_type else f"{relevant_time_units[1].title()}") # add
            percentile_right_axis.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda count, _: f"{int(count):,}")) # add commas
        if is_bottom_plot(plot_type = plot_type): # if bottom plot, add x labels
            axes[plot_type].set_xlabel("Percentile (%)")
        else: # is not a bottom plot
            # axes[plot_type].set_xticks([]) # will keep xticks for now
            axes[plot_type].set_xticklabels([])
        axes[plot_type].set_title(plot_type.title())
    
    # add a legend
    handles, labels = axes[plot_types[0]].get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    axes["legend"].legend(handles = by_label.values(), labels = list(map(lambda expressive_feature_type: split_camel_case(string = expressive_feature_type, sep = " ").title(), by_label.keys())), loc = "center", fontsize = "medium", title_fontsize = "large", alignment = "center", ncol = 2, title = "Expressive Feature", mode = "expand")
    axes["legend"].axis("off")
    # save image
    fig.savefig(output_filepaths[2], dpi = LARGE_PLOTS_DPI, transparent = True, bbox_inches = "tight") # save image
    logging.info(f"Second Sparsity Percentiles plot saved to {output_filepaths[2]}.")

    # clear up some memory
    del sparsity

    # return nothing
    return None

##################################################


# MAIN CLASS FOR MULTIPROCESSING
##################################################

if __name__ == "__main__":

    # CONSTANTS
    ##################################################

    # command-line arguments
    args = parse_args()

    # make sure files/directories exist
    if not exists(args.input_filepath):
        raise FileNotFoundError(f"{args.input_filepath} does not exist.")
    if not exists(args.file_output_dir):
        makedirs(args.file_output_dir)
    if not exists(args.output_dir):
        makedirs(args.output_dir)

    # output filepaths for data used in plots
    PLOT_TYPES = ("song_length", "density", "summary", "sparsity")
    PLOT_DATA_OUTPUT_FILEPATHS = {plot_type : f"{args.file_output_dir}/{plot_type}.csv" for plot_type in PLOT_TYPES}

    # set up logging
    logging.basicConfig(level = logging.INFO, format = "%(message)s")

    ##################################################


    # LOOK AT DISTRIBUTION OF EXPRESSIVE FEATURE TYPES IN "RICH" FILES
    ##################################################

    # if any of the plot data doesn't exist, create it
    if (not any(exists(path) for path in tuple(PLOT_DATA_OUTPUT_FILEPATHS.values()))) or args.reset:

        # load in data
        data = pd.read_csv(filepath_or_buffer = args.input_filepath, sep = ",", header = 0, index_col = False)

        # filter data
        data = data[data["in_dataset"] & (data["expressive_features"].apply(lambda expressive_features_path: exists(str(expressive_features_path))))]

        # create column names
        pd.DataFrame(columns = SONG_LENGTH_COLUMNS).to_csv(path_or_buf = PLOT_DATA_OUTPUT_FILEPATHS["song_length"], sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w") # song length
        pd.DataFrame(columns = DENSITY_COLUMNS).to_csv(path_or_buf = PLOT_DATA_OUTPUT_FILEPATHS["density"], sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w") # density
        pd.DataFrame(columns = FEATURE_TYPES_SUMMARY_COLUMNS).to_csv(path_or_buf = PLOT_DATA_OUTPUT_FILEPATHS["summary"], sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w") # features summary
        pd.DataFrame(columns = SPARSITY_COLUMNS).to_csv(path_or_buf = PLOT_DATA_OUTPUT_FILEPATHS["sparsity"], sep = ",", na_rep = NA_VALUE, header = True, index = False, mode = "w") # sparsity
        sparsity_pickled_percentiles_path = PLOT_DATA_OUTPUT_FILEPATHS["sparsity"].split(".")[0] + "_percentiles.pickle"
        if exists(sparsity_pickled_percentiles_path):
            remove(sparsity_pickled_percentiles_path) # remove pickled percentiles file

        # parse through data with multiprocessing
        logging.info(f"N_PATHS = {len(data)}") # print number of paths to process
        chunk_size = 1
        start_time = perf_counter() # start the timer
        with multiprocessing.Pool(processes = args.jobs) as pool:
            results = list(tqdm(iterable = pool.imap_unordered(func = extract_information, iterable = data["expressive_features"], chunksize = chunk_size), desc = "Summarizing Expressive Features", total = len(data)))
        end_time = perf_counter() # stop the timer
        total_time = end_time - start_time # compute total time elapsed
        total_time = strftime("%H:%M:%S", gmtime(total_time)) # convert into pretty string
        logging.info(f"Total time: {total_time}")   

    ##################################################


    # CREATE PLOTS
    ##################################################

    plot_output_filepaths = [f"{args.output_dir}/{plot_type}.pdf" for plot_type in PLOT_DATA_OUTPUT_FILEPATHS.keys()]
    song_length_statistics = get_song_length_statistics(input_filepath = PLOT_DATA_OUTPUT_FILEPATHS["song_length"])
    logging.info("Song Length Statistics:")
    for relevant_time_unit, statistics in song_length_statistics.items():
        logging.info(f"  = {relevant_time_unit.title()}:")
        for statistic_type, value in statistics.items():
            logging.info(f"    - {' '.join(statistic_type.split('_')).title()}: {value:,.2f}")
    _ = make_density_plot(input_filepath = PLOT_DATA_OUTPUT_FILEPATHS["density"], output_filepath = plot_output_filepaths[PLOT_TYPES.index("density")])
    expressive_feature_types = make_summary_plot(input_filepath = PLOT_DATA_OUTPUT_FILEPATHS["summary"], output_filepath_prefix = plot_output_filepaths[PLOT_TYPES.index("summary")].split(".")[0])
    _ = make_sparsity_plot(input_filepath = PLOT_DATA_OUTPUT_FILEPATHS["sparsity"], output_filepath_prefix = plot_output_filepaths[PLOT_TYPES.index("sparsity")].split(".")[0], expressive_feature_types = expressive_feature_types[::-1])

    ##################################################

##################################################
